# config/model.yaml

data:
  bin_dir: datasets
  bin_file: mel_spectrograms.h5
  mel_key: mel_spectrograms # Key for target mel spectrograms
  phoneme_key: phonemes     # Key for input phoneme sequences
  duration_key: durations   # Key for input duration sequences
  f0_key: f0                # Key for input F0 sequences
  lazy_load: true
  raw_dir: datasets/gin
  # max_samples: 100 # Set to null or remove to use full dataset
  sample_percentage: null
  variable_length: true
  min_phones_in_lab: 5 # Minimum number of phone entries required in a .lab file to process it

audio:
  sample_rate: 22050
  n_fft: 1024
  hop_length: 256
  win_length: 1024
  n_mels: 80
  fmin: 0
  fmax: 8000
  f0_min: 50
  f0_max: 600
  max_audio_length: 10.0

# Model specific parameters
model:
  name: SVSGAN # Updated model name
  mel_bins: 80        # Shared: Output dimension (should match audio.n_mels)
  f0_dim: 1           # Shared: Dimension of F0 input (usually 1)
  vocab_size: null    # Shared: Will be dynamically set from data

  generator:
    embedding_dim: 256 # Default from SVSGAN code
    rnn_hidden_dim: 512 # Default from SVSGAN code
    rnn_layers: 2       # Default from SVSGAN code
    bidirectional: true # Default from SVSGAN code

  # Post-Net Configuration (Used by Generator)
  postnet:
    n_convs: 5
    kernel_size: 5
    channels: 512
    dropout: 0.5 # Default from SVSGAN code

  discriminator:
    in_channels: 1      # Default from SVSGAN code
    n_layers: 4         # Default from SVSGAN code
    base_channels: 64   # Default from SVSGAN code
    kernel_size: [3, 9] # Default from SVSGAN code (Use list for tuple)
    stride: [1, 2]      # Default from SVSGAN code (Use list for tuple)
    padding: [1, 4]     # Default from SVSGAN code (Use list for tuple)
    norm_layer: 'instance' # Default from SVSGAN code ('batch' or 'instance')

# Training Configuration
train:
  batch_size: 16        # Adjust based on GPU memory
  epochs: 100           # GANs often require more epochs, adjust as needed
  val_interval: 1       # Run validation every N epochs
  log_dir: logs         # Base directory for all logs
  tensorboard_log_dir: tensorboard # Subdirectory for TensorBoard logs
  checkpoint_dir: checkpoints # Subdirectory for model checkpoints
  num_workers: 4        # DataLoader workers
  log_spectrogram_every_n_val_steps: 5 # Log spectrogram comparison every N validation steps

  # GAN specific training params
  learning_rate_g: 0.0004 # Generator learning rate
  learning_rate_d: 0.00005 # Discriminator learning rate (adjusted)
  adam_beta1: 0.5         # Adam beta1 (common for GANs)
  adam_beta2: 0.999      # Adam beta2 (common for GANs)
  lambda_recon: 50        # Weight for L1 reconstruction loss in Generator loss (adjusted)

  # log_interval defines log_every_n_steps in Trainer
  log_interval: 50        # Log training metrics every N steps

  # Removed: learning_rate (replaced by _g/_d), save_interval (handled by checkpoint callback)