data:
  bin_dir: datasets
  bin_file: mel_spectrograms.h5
  mel_key: mel_spectrogram # Key for target mel spectrograms 
  phoneme_key: phone_sequence # Key for input phoneme sequences
  duration_key: adjusted_durations # Key for input duration sequences
  f0_key: f0 # Key for input F0 sequences
  midi_pitch_key: midi_pitch_estimated # Key for estimated MIDI pitch
  lazy_load: true
  raw_dir: datasets/gin
  raw_audio_key: raw_audio # Key for the raw audio waveform
  # max_samples: 100 # Set to null or remove to use full dataset
  sample_percentage: null
  variable_length: true
  min_phones_in_lab: 5 # Minimum number of phone entries required in a .lab file to process it

audio:
  sample_rate: 22050
  n_fft: 1024
  hop_length: 256
  win_length: 1024
  n_mels: 80
  fmin: 0
  fmax: 8000
  f0_min: 50
  f0_max: 600
  max_audio_length: 10.0

# Model specific parameters
model:
  name: ProgressiveSVS # Upstream model name
  mel_bins: 80        # Shared: Output dimension (should match audio.n_mels)
  vocab_size: null    # Shared: Will be dynamically set from data

  # --- ProgressiveSVS Specific Parameters ---
  # Embedding dimensions
  f0_embed_dim: 64
  phone_embed_dim: 256
  midi_embed_dim: 64
  unvoiced_embed_dim: 16 # Dimension for unvoiced flag embedding
  # Resolution scaling factors (relative to full mel_bins)
  low_res_scale: 4
  mid_res_scale: 2

  # Channel configurations for each stage
  low_res_channels: [256, 256, 128]
  mid_res_channels: [128, 128, 64]
  high_res_channels: [64, 64, 32] # Updated channel config

  # --- Multi-band Config ---
  num_bands_stage1: 2
  num_bands_stage2: 3
  band_processing_stage2: 'separate' # Options: 'separate', 'shared'
  num_bands_stage3: 4
  band_processing_stage3: 'separate'
  # --- End Multi-band Config ---

  # Initial training stage (1, 2, or 3)
  current_stage: 1

vocoder:
  name: UNetVocoder  # Keep the same name for compatibility
  
  # === SIGNAL PROCESSING ENHANCEMENTS ===
  
  # 1. Phase Reconstruction Improvement
  use_phase_prediction: true  # Enable explicit phase modeling
  phase_loss_weight: 0.5      # Weight for phase reconstruction loss
  
  # 2. Perceptual Weighting
  use_perceptual_weighting: false   # Enable perceptual weighting
  perceptual_curve_type: 'a'       # Use A-weighting curve ('a', flat, or custom)
  
  # 3. Harmonic-Plus-Noise Model
  use_harmonic_plus_noise: false     # Enable harmonic + noise decomposition
  harmonic_ratio: 0.7               # Balance between harmonic and noise components
  harmonic_loss_weight: 0.6         # Weight for harmonic component loss
  noise_loss_weight: 0.4            # Weight for noise component loss
  harmonic_threshold: 0.2           # Threshold for harmonic band filters
  
  # 4. Multi-Resolution STFT Analysis
  use_multi_resolution_stft: true   # Use multiple STFT resolutions for loss
  
  # === END SIGNAL PROCESSING ENHANCEMENTS ===
  
  # U-Net architecture parameters (existing)
  encoder_channels: [32, 64, 96, 128]
  decoder_channels: [96, 64, 32, 32]
  kernel_size: 5
  
  # Enhanced architecture parameters (existing)
  use_enhanced_unet: true  # Set to false to use original UNet
  
  # Normalization settings
  norm_type: "instance"  # Options: "batch", "instance", "layer"
  
  # Residual connection settings
  use_residual_connections: true  # Enable residual connections
  
  # Attention settings
  use_attention: true  # Enable self-attention in bottleneck
  attention_reduction: 8  # Reduction factor for attention (higher = less computation)
  
  # Non-adjacent skip connections
  use_non_adjacent_skips: true  # Enable connections between distant layers
  
  # Audio generation parameters (existing)
  noise_scale: 0.6  # Scale factor for noise during inference
  use_f0_conditioning: true  # Enable F0 conditioning for the vocoder
  
  # For different chunk-based processing (existing)
  chunk_size_seconds: 2.5  # Process audio in chunks of this duration

# Training Configuration
train:
  batch_size: 16        # Adjust based on GPU memory
  unvoiced_weight: 0.5  # Weight for unvoiced frames in loss calculation
  epochs_per_stage:
    stage1: 150
    stage2: 120
    stage3: 100
  
  val_interval: 1       # Run validation every N epochs
  log_dir: logs         # Base directory for all logs
  tensorboard_log_dir: logs # Subdirectory for TensorBoard logs
  checkpoint_dir: logs/checkpoints # Subdirectory for model checkpoints
  num_workers: 4        # DataLoader workers
  log_spectrogram_every_n_val_steps: 5 # Log spectrogram comparison every N validation steps

  # --- Stage-Specific Learning Rates ---
  learning_rate_per_stage:
    stage1: 0.0005  # Example LR for stage 1
    stage2: 0.0007  # Example LR for stage 2
    stage3: 0.0009  # Example LR for stage 3
  # --- End Stage-Specific Learning Rates ---

  # --- Global Optimizer/Scheduler Params ---
  weight_decay: 0.0001    # Adam weight decay
  lr_factor: 0.5          # Factor by which the learning rate will be reduced (scheduler)
  lr_patience: 10         # Number of epochs with no improvement after which learning rate will be reduced (scheduler)
  gradient_clip_val: 0.5  # Gradient clipping value
  freeze_loaded_weights: false # Default: Freeze weights when loading from ckpt

  # log_interval defines log_every_n_steps in Trainer
  log_interval: 50        # Log training metrics every N steps

  # --- Vocoder Training Specific Params ---
  vocoder_loss: 'Combined' # UPDATED: Reflects all loss components
  loss_lambda_td: 0.7     # Weight for time-domain (L1) loss component
  
  # STFT loss parameters (now used in multi-resolution framework)
  stft_fft_size: 1024     # Base FFT size for STFT loss
  stft_hop_size: 120      # Base hop size for STFT loss
  stft_win_length: 600    # Base window length for STFT loss
  loss_lambda_sc: 0.7     # Weight for spectral convergence loss component
  loss_lambda_mag: 0.8    # Weight for log STFT magnitude loss component
  loss_lambda_mel: 0.7    # Weight for Mel Spectrogram Reconstruction loss
  
  vocoder_max_epochs: 1000 # Maximum epochs for vocoder training
  vocoder_learning_rate: 0.00001 # Learning rate for vocoder training
  
  # --- Vocoder Logging ---
  log_vocoder_audio_epoch_interval: 5 # Log audio every N validation epochs
  log_n_vocoder_audio_samples: 2      # Number of audio samples to log from validation batch
  log_vocoder_spectrograms: true      # Whether to log spectrogram comparison plots